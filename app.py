# ðŸŒ ë¹„ì²´ê³„ì  ìœ„í—˜ ë¶„ì„ Streamlit ì•± (í™•ìž¥ëœ í‚¤ì›Œë“œ í¬í•¨)
import streamlit as st
import requests
import re
import pandas as pd
import datetime
import plotly.express as px
from transformers import BertTokenizer, BertForSequenceClassification, pipeline

# ë³´ê°•ëœ ìœ„í—˜ í‚¤ì›Œë“œ
def get_risk_keywords():
    return {
        # ê¸°ì¡´ + í™•ìž¥ (ì´ 80ê°œ ì´ìƒ)
        "ì¹¨ê³µ": 1.5, "ì „ìŸ": 1.6, "ë‚´ì „": 1.5, "í­íƒ„": 1.5, "êµ°ì‚¬ í–‰ë™": 1.6,
        "ê°ˆë“±": 1.4, "ë¶„ìŸ": 1.4, "í…ŒëŸ¬": 1.7, "ì •ë³€": 1.5, "ë…ìž¬": 1.3,
        "ì •ì¹˜ ë¶ˆì•ˆ": 1.6, "ì¿ ë°íƒ€": 1.7, "ë´‰ì‡„": 1.4, "ê²€ì—´": 1.3, "ì‚¬ì´ë²„ ê³µê²©": 1.6,
        "ë³´ì•ˆ ì¹¨í•´": 1.6, "í•´í‚¹": 1.5, "í”¼ì‹±": 1.5, "ëžœì„¬ì›¨ì–´": 1.6, "ì•…ì„±ì½”ë“œ": 1.5,
        "ì¹¨í•´": 1.5, "ì •ë³´ ìœ ì¶œ": 1.6, "ê°œì¸ì •ë³´ ìœ ì¶œ": 1.7, "ë°ì´í„° ìœ ì¶œ": 1.6,
        "íŒŒì‚°": 1.6, "ì‹¤ì—…": 1.5, "ì •ë¦¬í•´ê³ ": 1.5, "í‡´ì¶œ": 1.4, "ì±„ë¬´ë¶ˆì´í–‰": 1.7,
        "ì ìž": 1.3, "ë””í´íŠ¸": 1.7, "ë¶€ë„": 1.6, "ë¶ˆí™©": 1.5, "ê²½ê¸° ì¹¨ì²´": 1.5,
        "ê¸´ì¶•": 1.3, "ê¸ˆë¦¬ ì¸ìƒ": 1.3, "í™˜ìœ¨ ë¶ˆì•ˆ": 1.4, "ì¸í”Œë ˆì´ì…˜": 1.4, "ìŠ¤íƒœê·¸í”Œë ˆì´ì…˜": 1.5,
        "ë¬¼ê°€ ìƒìŠ¹": 1.3, "ìƒì‚° ì°¨ì§ˆ": 1.4, "ìˆ˜ê¸‰ ë¶ˆê· í˜•": 1.4, "ê³µê¸‰ë§ ë¶•ê´´": 1.6,
        "í­ë™": 1.5, "ì‹œìœ„": 1.3, "ë¶ˆë²•": 1.2, "ì‚¬ê¸°": 1.4, "íš¡ë ¹": 1.4,
        "ì¡°ìž‘": 1.5, "ë¶€íŒ¨": 1.5, "ë¶ˆê³µì •": 1.3, "ë¶ˆë²• í–‰ìœ„": 1.5, "ì˜í˜¹": 1.3,
        "ê¸°ì†Œ": 1.2, "ì²´í¬": 1.3, "ì§•ì—­": 1.2, "í˜ì˜": 1.2,
        "ë¦¬ì½œ": 1.3, "ê²°í•¨": 1.4, "ê³ ìž¥": 1.2, "ìœ„í—˜": 1.3, "ë¶ˆëŸ‰": 1.3,
        "ë¶•ê´´": 1.4, "ì¹¨ëª°": 1.5, "í™”ìž¬": 1.4, "ê°ì—¼ë³‘": 1.5, "ì „ì—¼ë³‘": 1.5,
        "ì§ˆë³‘": 1.3, "íŒ¬ë°ë¯¹": 1.6, "í™•ì‚°": 1.3, "ì˜ë£Œ ë¶•ê´´": 1.5,
        "ë¶ˆë§¤": 1.3, "íƒˆí‡´": 1.2, "ì œìž¬": 1.4, "ê³ ë¦½": 1.3,
        "disruption": 1.5, "fraud": 1.4, "shutdown": 1.4, "crisis": 1.6, "layoff": 1.5,
        "protest": 1.3, "riot": 1.6, "collapse": 1.6, "default": 1.7, "sanctions": 1.5
    }

# ë³´ê°•ëœ ê¸ì • í‚¤ì›Œë“œ
POSITIVE_KEYWORDS = {
    "ì„±ìž¥": -0.4, "íšŒë³µ": -0.5, "ê°œì„ ": -0.4, "ì•ˆì •": -0.4, "í˜‘ë ¥": -0.3,
    "í‰í™”": -0.6, "í•©ì˜": -0.3, "ì™¸êµ": -0.3, "ì •ìƒíšŒë‹´": -0.3, "í˜‘ìƒ": -0.3,
    "ì§€ì›": -0.4, "ìž¬ê±´": -0.4, "íˆ¬ìž": -0.5, "ìˆ˜ì¶œ ì¦ê°€": -0.5, "ë¬´ì—­ í™•ëŒ€": -0.5,
    "ê²½ì œ íšŒë³µ": -0.5, "ê¸°ìˆ  í˜ì‹ ": -0.5, "ì„±ê³µ": -0.5, "ë°œì „": -0.4,
    "ê¸°íšŒ": -0.4, "ì‹ ë¢°": -0.3, "ê¸ì •": -0.3, "ì•ˆì •ì„¸": -0.4, "íšŒë³µì„¸": -0.4,
    "ì„ ì§„í™”": -0.3, "ê°œë°©": -0.3, "ì™„í™”": -0.3, "í˜¸ì „": -0.3, "ê¸ì • í‰ê°€": -0.3,
    "í•©ìž‘": -0.3, "ìƒìƒ": -0.3, "ìœ ì¹˜": -0.3, "ì°½ì—…": -0.4, "ì¼ìžë¦¬ ì°½ì¶œ": -0.4,
    "ê¸°ìˆ  ê°œë°œ": -0.4, "í˜ì‹ ": -0.5, "ì‹ ê¸°ìˆ ": -0.4, "ì¹œí™˜ê²½": -0.3, "ì§€ì† ê°€ëŠ¥": -0.4,
    "ë¦¬ë”ì‹­": -0.3, "ë¯¼ì£¼ì£¼ì˜": -0.2, "ìžìœ¨ì„±": -0.3, "ê³µì •ì„±": -0.2, "ì²­ë ´": -0.2,
    "ì˜ë£Œ ê°œì„ ": -0.4, "ë°±ì‹  í™•ë³´": -0.4, "ì•ˆì „ë§": -0.4,
    "breakthrough": -0.5, "recovery": -0.5, "growth": -0.4, "improvement": -0.4,
    "stability": -0.4, "cooperation": -0.3, "diplomacy": -0.3, "innovation": -0.5,
    "opportunity": -0.4, "peace": -0.5, "support": -0.4
}

@st.cache_resource
def load_sentiment_model():
    tokenizer = BertTokenizer.from_pretrained('monologg/kobert')
    model = BertForSequenceClassification.from_pretrained('beomi/kcbert-base', num_labels=3)
    return pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

def calculate_risk_score(text, keywords):
    text_lower = text.lower()
    score = 0
    for word, weight in keywords.items():
        count = len(re.findall(re.escape(word), text_lower, re.IGNORECASE))
        score += weight * count
    return score

def kobert_sentiment(pipeline, text):
    result = pipeline(text[:512])[0]
    label = result['label']
    score = result['score']
    if label == 'LABEL_0': return -score
    elif label == 'LABEL_2': return score
    return 0

# UI ì‹œìž‘
st.set_page_config(page_title="Unsystematic Risk Analyzer", layout="wide")
st.title("ðŸ“‰ ë‰´ìŠ¤ ê¸°ë°˜ ë¹„ì²´ê³„ì  ìœ„í—˜ ë¶„ì„ê¸°")

keyword = st.text_input("ðŸ” ë‰´ìŠ¤ í‚¤ì›Œë“œ", "ì•„ë¥´í—¨í‹°ë‚˜ ì •ì¹˜ ë¶ˆì•ˆ")
api_key = st.text_input("ðŸ”‘ GNews API Key", type="password")
custom_article = st.text_area("âœï¸ ë¶„ì„í•  ê¸°ì‚¬ í…ìŠ¤íŠ¸ ì§ì ‘ ìž…ë ¥ (ì„ íƒ)", height=300)

if st.button("ðŸš€ ë¶„ì„ ì‹œìž‘"):
    with st.spinner("ëª¨ë¸ ë¡œë”© ì¤‘..."):
        nlp = load_sentiment_model()

    if custom_article.strip():
        articles = [custom_article.strip()]
    else:
        url = f"https://gnews.io/api/v4/search?q={keyword}&lang=ko&token={api_key}"
        r = requests.get(url)
        articles = [a['title'] + ' ' + (a['description'] or '') for a in r.json().get('articles', [])]

    if not articles:
        st.warning("ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        rows = []
        risk_dict = get_risk_keywords()
        for t in articles:
            base_score = calculate_risk_score(t, risk_dict)
            bonus = calculate_risk_score(t, POSITIVE_KEYWORDS)
            final_score = base_score + bonus
            polarity = kobert_sentiment(nlp, t)
            score = round(final_score * (1 - polarity), 2)
            rows.append({"í…ìŠ¤íŠ¸": t, "ìœ„í—˜ ì ìˆ˜": final_score, "ê°ì„± ì ìˆ˜": round(polarity, 2), "ìµœì¢… ìœ„í—˜ ì§€ìˆ˜": score,
                         "ì‹œê°„": datetime.datetime.now()})

        df = pd.DataFrame(rows)
        st.success(f"ì´ {len(df)}ê°œ ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ!")
        st.dataframe(df.sort_values("ìµœì¢… ìœ„í—˜ ì§€ìˆ˜", ascending=False))

        # ì‹œê°í™”
        st.subheader("ðŸ“Š ì‹œê°í™” ê²°ê³¼")
        st.plotly_chart(px.line(df, x="ì‹œê°„", y="ìµœì¢… ìœ„í—˜ ì§€ìˆ˜", title="ì‹œê°„ë³„ ìœ„í—˜ ì¶”ì„¸"))
        st.plotly_chart(px.histogram(df, x="ìµœì¢… ìœ„í—˜ ì§€ìˆ˜", nbins=15, title="ìœ„í—˜ ë¶„í¬"))
        st.plotly_chart(px.scatter(df, x="ê°ì„± ì ìˆ˜", y="ìœ„í—˜ ì ìˆ˜", color="ìµœì¢… ìœ„í—˜ ì§€ìˆ˜", size="ìµœì¢… ìœ„í—˜ ì§€ìˆ˜",
                                   hover_data=["í…ìŠ¤íŠ¸"], title="ê°ì„± vs ìœ„í—˜ ì ìˆ˜ ì‚°ì ë„"))

        top5 = df.sort_values("ìµœì¢… ìœ„í—˜ ì§€ìˆ˜", ascending=False).head(5)
        st.subheader("ðŸ”¥ ê³ ìœ„í—˜ ê¸°ì‚¬ Top 5")
        for i, row in top5.iterrows():
            st.markdown(f"**{i+1}. ìœ„í—˜ë„: {row['ìµœì¢… ìœ„í—˜ ì§€ìˆ˜']} / ê°ì„±: {row['ê°ì„± ì ìˆ˜']}**")
            st.caption(row['í…ìŠ¤íŠ¸'][:400] + ("..." if len(row['í…ìŠ¤íŠ¸']) > 400 else ""))

        st.download_button("ðŸ“¥ CSV ë‹¤ìš´ë¡œë“œ", df.to_csv(index=False).encode(), "risk_analysis.csv", "text/csv")